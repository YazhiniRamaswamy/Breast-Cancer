{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "\n",
    "# linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Algorithms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BC_data = pd.read_csv(\"C:/Users/yazhini/Bcancer_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of Dataset\n",
    "BC_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 rows from Dataset\n",
    "BC_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the dataset and check if there are any null values in each column\n",
    "BC_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the column 'unnamed'\n",
    "BC_data = BC_data.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dimension of Dataset\n",
    "BC_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the column names based on Mean, Standard error or worst**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Column\n",
    "BC_data_mean = BC_data.loc[:,[\"diagnosis\", \"radius_mean\", \"texture_mean\",\"perimeter_mean\",\n",
    "                             \"area_mean\", \"smoothness_mean\",\"compactness_mean\", \"concavity_mean\",\n",
    "                             \"symmetry_mean\",\"fractal_dimension_mean\",\"concave points_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the BC_data_mean data\n",
    "BC_data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Error(se) Column\n",
    "BC_data_se = BC_data.loc[:,[\"diagnosis\", \"radius_se\", \"texture_se\",\n",
    "            \"perimeter_se\", \"area_se\", \"smoothness_se\", \"compactness_se\", \n",
    "            \"concavity_se\", \"concave points_se\", \"symmetry_se\", \n",
    "            \"fractal_dimension_se\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the BC_data_se data\n",
    "BC_data_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column name ends with 'worst'\n",
    "BC_data_worst = BC_data.loc[:,[\"diagnosis\", \"radius_worst\", \n",
    "               \"texture_worst\",\"perimeter_worst\", \"area_worst\",                                \n",
    "               \"smoothness_worst\", \"compactness_worst\", \"concavity_worst\",\n",
    "               \"concave points_worst\", \"symmetry_worst\", \n",
    "               \"fractal_dimension_worst\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the BC_data_worst data\n",
    "BC_data_worst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization/Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of Malignant and Benign in diagnosis column\n",
    "BC_data[\"diagnosis\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise the count of values in diagnosis column\n",
    "sns.countplot(BC_data[\"diagnosis\"], label = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names in three list\n",
    "list_worst=[\"diagnosis\", \"radius_worst\", \"texture_worst\",\n",
    "           \"perimeter_worst\", \"area_worst\", \"smoothness_worst\",               \n",
    "           \"compactness_worst\", \"concavity_worst\",\"concave points_worst\", \n",
    "            \"symmetry_worst\",\"fractal_dimension_worst\" ]\n",
    "list_se=[\"diagnosis\", \"radius_se\", \"texture_se\",\n",
    "        \"perimeter_se\", \"area_se\", \"smoothness_se\", \"compactness_se\", \n",
    "        \"concavity_se\", \"concave points_se\", \"symmetry_se\", \n",
    "        \"fractal_dimension_se\"]\n",
    "list_mean = [\"diagnosis\", \"radius_mean\", \"texture_mean\",\"perimeter_mean\",\n",
    "            \"area_mean\", \"smoothness_mean\",\"compactness_mean\", \n",
    "             \"concavity_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\n",
    "             \"concave points_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise each feature in 'worst' group by diagnostic result\n",
    "for i in range(1,11): \n",
    "    \n",
    "    grid = sns.FacetGrid(BC_data_worst, hue='diagnosis')\n",
    "    grid.map(plt.hist,list_worst[i], \n",
    "                alpha=.5, bins=10)\n",
    "    grid.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise each feature in 'mean' group by diagnostic result\n",
    "for i in range(1,11):    \n",
    "    grid = sns.FacetGrid(BC_data_mean, hue='diagnosis')\n",
    "    grid.map(plt.hist,list_mean[i], \n",
    "                alpha=.5, bins=10)\n",
    "    grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise each feature in 'se' group by diagnostic result\n",
    "for i in range(1,11):    \n",
    "    grid = sns.FacetGrid(BC_data_se, hue='diagnosis')\n",
    "    grid.map(plt.hist,list_se[i], \n",
    "                alpha=.5, bins=10)\n",
    "    grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the correlation among the variables except id, target\n",
    "correlation = BC_data.iloc[:,2:32].corr()\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,15))\n",
    "sns.heatmap(correlation, annot = True,fmt = '.0%') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of correlation dataset\n",
    "size=correlation.shape[0]\n",
    "size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the variables with more than 0.9 correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all features except id and diagnosis \n",
    "BC_Data1=BC_data.iloc[:,2:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features which has .9 or more correlation\n",
    "total_column = np.full((size,), True, dtype = bool)\n",
    "for x in range(size):\n",
    "    for y in range(x+1, size):\n",
    "        if correlation.iloc[x,y]>= 0.9:\n",
    "            print(x,y)\n",
    "            print(correlation.iloc[x,y])\n",
    "            if total_column[y]:\n",
    "                total_column[y]= False\n",
    "filtered_columns = BC_Data1.columns[total_column]\n",
    "final_data = BC_data[filtered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispaly correlation for 20 variables which we get after reduction\n",
    "corr_after_reduction =final_data.corr()\n",
    "corr_after_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of 20 features which we get for final analysis\n",
    "filtered_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dimension of data after reduction\n",
    "corr_after_reduction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the correlation with heat map for final data which has 20 feature\n",
    "plt.figure(figsize = (15,15))\n",
    "sns.heatmap(corr_after_reduction, annot = True,fmt = '.0%') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame for target column\n",
    "target = pd.DataFrame()\n",
    "target['diagnosis'] = BC_data.iloc[:,1]\n",
    "target['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all 20 features with respect to diagnosis\n",
    "fig = plt.figure(figsize = (20, 25))\n",
    "y = 0\n",
    "for x in final_data.columns:\n",
    "    plt.subplot(7, 3, y+1)\n",
    "    y += 1\n",
    "    sns.distplot(final_data[x][target['diagnosis']=='M'], color='blue', label = 'Benign')\n",
    "    sns.distplot(final_data[x][target['diagnosis']==\"B\"], color='red', label = 'Malignant')\n",
    "    plt.legend(loc='best')\n",
    "fig.suptitle('Feature Analysis with Diagnosis')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the categorical data values\n",
    "lblencoder_d= LabelEncoder()\n",
    "BC_data.iloc[:,1] = lblencoder_d.fit_transform(BC_data.iloc[:,1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into independent(predictors) and dependent (target)\n",
    "X=final_data.values\n",
    "Y=BC_data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardisation\n",
    "Scale all numerical features in X using sklearn's StandardScaler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform standardisation\n",
    "datascaled = StandardScaler()\n",
    "X= datascaled.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "PCA is used for dimentionality reduction and to know the variance of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "out = pca.fit_transform(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of components based on cumulative variance\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(range(1,21),pca.explained_variance_ratio_.cumsum(), marker='o',linestyle='--')\n",
    "plt.title('Explanied variance by Componentes')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative')\n",
    "plt.axhline(y=.95, xmin=0, xmax=20,color='red')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen 10 principal components to explain more than 0.95 of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCA(n_components = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pca1.transform(X)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame with 10 component and assign name for each component\n",
    "pca_df = pd.DataFrame( data =  final,\n",
    "                    columns = ['pc1', 'pc2','pc3', 'pc4','pc5','pc6','pc7','pc8','pc9','pc10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach the diagnosis column to PCA new data frame\n",
    "pca_df['Response'] = BC_data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dimension \n",
    "pca_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top rows\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatter plot between Pc1 and Pc2 component since it explains 55% of information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize component1 and component2\n",
    "sns.scatterplot('pc1','pc2',hue='Response',data=pca_df,palette=['g','r'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To visualize which variables are the most influential on the first 2 components**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biplot(score,coeff,labels=None):\n",
    "    s1 = score[:,0]\n",
    "    s2 = score[:,1]\n",
    "    n=coeff.shape[0]\n",
    "    scalex = 1.0/(s1.max()- s1.min())\n",
    "    scaley = 1.0/(s2.max()- s2.min())\n",
    "    plt.scatter(s1 * scalex,s2 * scaley, c = pca_df.iloc[:,-1])\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color='r',alpha=.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color='g', ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color='g', ha='center', va='center')\n",
    " \n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    " \n",
    "# test\n",
    " \n",
    "#data=np.random.rand(100,10)*10\n",
    "#pca=PCA(scores)\n",
    " \n",
    "biplot(final[:,0:2],np.transpose(pca1.components_[0:2, :]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the importance of each feature is reflected by the magnitude of the corresponding values in the eigenvectors (higher magnitude - higher importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the PC explained variance.\n",
    "pca1.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC1 explains 42% of the information and PC2 13% of information. Together,  PC1 and PC2 only explain 55% of information.\n",
    "\n",
    "Now, let's find the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each variable's variance in each component\n",
    "abs(pca1.components_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pca1.components_ has shape (row=n_components, column=n_features). Thus, by looking at the PC1 (First Principal Component) which is the first row: (1.32037515e-01 8.65416182e-02 2.26589267e-01 3.19548976e-01\n",
    "3.01093179e-01 2.27708080e-01 2.23447406e-01 1.69612240e-01 \n",
    "5.36124163e-02 9.74162025e-02 2.83931331e-01 2.52462094e-01\n",
    "2.50535394e-01 1.21786697e-01 2.29604985e-01 2.05937616e-01\n",
    "2.88112018e-01 2.89154562e-01 1.92610186e-01 2.59331142e-01) we can conclude that feature 1, 3 and 4 (or Var 1, 3 and 4 in the biplot) are the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign total dimension of PCA data frame to the new variable\n",
    "n_pcs= pca1.components_.shape[0]\n",
    "print('Dimension of pca data frame : ',n_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_index = [np.abs(pca1.components_[i]).argmax() for i in range(n_pcs)]\n",
    "print('Index of most important feature are :  ', most_important_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign name of each feature to list\n",
    "feature_names = ['radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean',\n",
    "       'concavity_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "       'radius_se', 'texture_se', 'smoothness_se', 'compactness_se',\n",
    "       'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'smoothness_worst', 'compactness_worst',\n",
    "       'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_names = [feature_names[most_important_index[i]] for i in range(n_pcs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame(important_dic.items())\n",
    "print('Most Important Feature in Each Component')\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the column into dependent nd independent for pca data frame\n",
    "X = pca_df.iloc[:,0:10]\n",
    "\n",
    "#Y will tell us if a patient has cancer or not\n",
    "Y = pca_df.iloc[:,-1].values\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data into 70% of training data and 30% of test data for both dependent and independent variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X ,Y, test_size = 0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dimension of X_train : ',X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dimension of X_test : ',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dimension of Y_train : ',Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dimension of Y_test : ',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function for the models\n",
    "def models(X_train, Y_train):\n",
    "    list_accuracy =[]\n",
    "    \n",
    "    #Mode0 --> Logistic regression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    log = LogisticRegression(random_state = 0)\n",
    "    log.fit(X_train, Y_train)\n",
    "    list_accuracy.append(log.score(X_train, Y_train))\n",
    "    \n",
    "    #Model1 --> Decision Tree\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "    tree.fit(X_train, Y_train)\n",
    "    list_accuracy.append(tree.score(X_train, Y_train))\n",
    "             \n",
    "    #Model2 --> Random Forest Classifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    forest.fit(X_train, Y_train)\n",
    "    list_accuracy.append(forest.score(X_train, Y_train))\n",
    "    \n",
    "    #Model3 --> kNeighbors\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    KNN = KNeighborsClassifier(n_neighbors=5, p=2, weights='distance')\n",
    "    KNN.fit(X_train, Y_train)\n",
    "    list_accuracy.append(KNN.score(X_train, Y_train))\n",
    "    \n",
    "    #Model4 --> SVM\n",
    "    from sklearn.svm import SVC\n",
    "    svc = SVC(gamma=0.025, C=3)\n",
    "    svc.fit(X_train,Y_train)\n",
    "    list_accuracy.append(svc.score(X_train, Y_train))\n",
    "    \n",
    "    #Print the result of the models\n",
    "    print('1) Logistic Regression Training Accuracy:', log.score(X_train, Y_train))\n",
    "    print('2) DecisionTreeClaassifier Training Accuracy:', tree.score(X_train, Y_train))           \n",
    "    print('3) Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train))\n",
    "    print('4) kNeighbors Training Accuracy:', KNN.score(X_train, Y_train))\n",
    "    print('5) Support Vector Machine Training Accuracy:', svc.score(X_train, Y_train)) \n",
    "    return log, tree, forest, KNN, svc,list_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign all model names to list\n",
    "model_names =['Logistic Regression','DecisionTreeClaassifier','Random Forest Classifier','kNeighbors Training','Support Vector Machine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model accuracy on test data on confusion matrix (method 2)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i in range( len(model) -1):\n",
    "    print('*'*30,model_names[i],'*'*30)\n",
    "    print('Confusion Matrix and Statistics')\n",
    "    cm = confusion_matrix(Y_test,model[i].predict(X_test))\n",
    "    #y_score = model[i].decision_function(X_test)\n",
    "    print(cm)\n",
    "    TP = cm[0][0]\n",
    "    TN = cm[1][1]\n",
    "    FN = cm[1][0]\n",
    "    FP = cm[0][1]\n",
    "\n",
    "    accuracy = (TP + TN)/(TP + TN + FN + FP)\n",
    "    #test_acc.append(accuracy)    \n",
    "    #print(model_names[i],'Tesing Accuracy =', accuracy)    \n",
    "    r = classification_report(Y_test, model[i].predict(X_test))\n",
    "    print (r)\n",
    "    \n",
    "#     sensitivity1 = TP/(TP+FP)\n",
    "#     print('Sensitivity :     ', sensitivity1 )\n",
    "\n",
    "    specificity1 = TN/(FN+TN)\n",
    "    print('Specificity :     ', specificity1)    \n",
    "    \n",
    "    print('Testing Accuracy   ',accuracy_score(Y_test, model[i].predict(X_test)))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
